{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MARKET_SHARE = 0.05 # from \n",
    "AVERAGE_HOUSEHOLD_SIZE = 2.5 # US average according to ...\n",
    "# recommended default baseline (actuarial starting point). Tune / replace with real data if available.\n",
    "BASE_RATE = 1 / 18  # US average annual claim probability per property (sensible starting point)\n",
    "NUM_TRIALS = 10000  # Monte Carlo draws per county/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import gamma\n",
    "from scipy.integrate import quad\n",
    "from numpy.linalg import cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counties for Florida:\n",
      "     COUNTY  insured_properties  lambda_base  severity_mean\n",
      "0   Alachua         6177.422222   343.190123  196545.729472\n",
      "1     Baker          618.933333    34.385185  137107.148543\n",
      "2       Bay         3885.977778   215.887654  712090.788201\n",
      "3  Bradford          627.400000    34.855556  239810.046710\n",
      "4   Brevard        13468.155556   748.230864  553818.713457\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell A – Data Loading + Preprocessing\n",
    "# ----------------------------\n",
    "\n",
    "# Relevant property information\n",
    "FLORIDA = {\n",
    "    \"name\": \"Florida\",\n",
    "    \"average_property_cost\": 325000, \n",
    "}\n",
    "WASHINGTON = {\n",
    "    \"name\": \"Washington\",\n",
    "    \"average_property_cost\": 610000, \n",
    "}\n",
    "\n",
    "# Load FEMA CSVs\n",
    "FLORIDA['df'] = pd.read_csv('fl_county.csv')\n",
    "WASHINGTON['df'] = pd.read_csv('wa_county.csv')\n",
    "\n",
    "# ----------------------------\n",
    "# CLEAN + PREPARE FEMA DATA\n",
    "# ----------------------------\n",
    "def clean_fema_df(state_dict):\n",
    "    df = state_dict[\"df\"].copy()\n",
    "\n",
    "    # Keep only the necessary columns\n",
    "    needed = [\"COUNTY\", \"POPULATION\", \"BUILDVALUE\", \"EAL_VALT\", \"RISK_VALUE\", \"RISK_SCORE\"]\n",
    "    df = df[needed]\n",
    "\n",
    "    # Rename for clarity\n",
    "    df = df.rename(columns={\n",
    "        \"BUILDVALUE\": \"exposure\",  # total building value\n",
    "        \"EAL_VALT\": \"EAL\",         # total expected annual loss\n",
    "        \"POPULATION\": \"population\"\n",
    "    })\n",
    "\n",
    "    # Convert numeric\n",
    "    df[\"exposure\"] = pd.to_numeric(df[\"exposure\"], errors=\"coerce\")\n",
    "    df[\"EAL\"] = pd.to_numeric(df[\"EAL\"], errors=\"coerce\")\n",
    "    df[\"population\"] = pd.to_numeric(df[\"population\"], errors=\"coerce\")\n",
    "    df[\"RISK_VALUE\"] = pd.to_numeric(df[\"RISK_VALUE\"], errors=\"coerce\")\n",
    "    df[\"RISK_SCORE\"] = pd.to_numeric(df[\"RISK_SCORE\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with missing exposure or EAL\n",
    "    df = df.dropna(subset=[\"exposure\", \"EAL\"])\n",
    "\n",
    "    state_dict[\"df_clean\"] = df\n",
    "    return state_dict\n",
    "\n",
    "FLORIDA = clean_fema_df(FLORIDA)\n",
    "WASHINGTON = clean_fema_df(WASHINGTON)\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocessing for GLM / Monte Carlo\n",
    "# ----------------------------\n",
    "\n",
    "def preprocess_location_df(location):\n",
    "    \"\"\"\n",
    "    Produces county-level variables for frequency × severity modeling:\n",
    "    - insured_properties: population / average household size\n",
    "    - lambda_base: expected claims per county based on empirical claim rate (~1/18)\n",
    "    - severity_mean: EAL per expected claim\n",
    "    - risk_scaled: normalized FEMA RISK_VALUE\n",
    "    \"\"\"\n",
    "    df = location[\"df_clean\"].copy()\n",
    "\n",
    "    # Estimate # of properties\n",
    "    df[\"properties\"] = df[\"population\"] / AVERAGE_HOUSEHOLD_SIZE\n",
    "\n",
    "    # Assume ~1/18 of properties claim per year\n",
    "    base_claim_rate = 1/18\n",
    "    df[\"insured_properties\"] = df[\"properties\"] * base_claim_rate\n",
    "\n",
    "    # baseline lambda = insured properties * empirical base rate\n",
    "    df[\"lambda_base\"] = df[\"insured_properties\"] * BASE_RATE\n",
    "\n",
    "\n",
    "    # severity per claim\n",
    "    df[\"severity_mean\"] = df[\"EAL\"] / df[\"lambda_base\"]\n",
    "\n",
    "    # normalized risk factor\n",
    "    df[\"risk_scaled\"] = df[\"RISK_VALUE\"] / df[\"RISK_VALUE\"].max()\n",
    "\n",
    "    location[\"df_glm\"] = df\n",
    "    return df\n",
    "\n",
    "FLORIDA[\"df_glm\"] = preprocess_location_df(FLORIDA)\n",
    "WASHINGTON[\"df_glm\"] = preprocess_location_df(WASHINGTON)\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Sample counties for Florida:\")\n",
    "print(FLORIDA[\"df_glm\"][[\"COUNTY\", \"insured_properties\", \"lambda_base\", \"severity_mean\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>insured_properties</th>\n",
       "      <th>lambda</th>\n",
       "      <th>severity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alachua</td>\n",
       "      <td>6177.422222</td>\n",
       "      <td>348.418918</td>\n",
       "      <td>187458.402435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baker</td>\n",
       "      <td>618.933333</td>\n",
       "      <td>32.597183</td>\n",
       "      <td>148870.611203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bay</td>\n",
       "      <td>3885.977778</td>\n",
       "      <td>223.988312</td>\n",
       "      <td>698468.183451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bradford</td>\n",
       "      <td>627.400000</td>\n",
       "      <td>36.494751</td>\n",
       "      <td>209217.807032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brevard</td>\n",
       "      <td>13468.155556</td>\n",
       "      <td>675.239627</td>\n",
       "      <td>491055.146773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     COUNTY  insured_properties      lambda  severity_mean\n",
       "0   Alachua         6177.422222  348.418918  187458.402435\n",
       "1     Baker          618.933333   32.597183  148870.611203\n",
       "2       Bay         3885.977778  223.988312  698468.183451\n",
       "3  Bradford          627.400000   36.494751  209217.807032\n",
       "4   Brevard        13468.155556  675.239627  491055.146773"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell B\n",
    "\n",
    "def generate_county_level_parameters(location, perturb_std_lambda=0.05, perturb_std_severity=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Produce county-level stochastic frequency (lambda) and severity.\n",
    "    \n",
    "    Args:\n",
    "        location: dict containing df_clean / df_glm\n",
    "        perturb_std_lambda: relative std dev for lambda perturbation (5% default)\n",
    "        perturb_std_severity: relative std dev for severity perturbation (10% default)\n",
    "        random_state: seed for reproducibility\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = location[\"df_glm\"].copy()\n",
    "    \n",
    "    # --- Perturb lambda around base ---\n",
    "    df[\"lambda\"] = df[\"lambda_base\"] * (1 + rng.normal(0, perturb_std_lambda, size=len(df)))\n",
    "    df[\"lambda\"] = df[\"lambda\"].clip(lower=0.01)  # prevent negative lambda\n",
    "\n",
    "    # --- Perturb severity around mean ---\n",
    "    df[\"severity_mean\"] = df[\"severity_mean\"] * (1 + rng.normal(0, perturb_std_severity, size=len(df)))\n",
    "    df[\"severity_mean\"] = df[\"severity_mean\"].clip(lower=1000)  # prevent absurdly low severity\n",
    "\n",
    "    \n",
    "    location[\"df_sim\"] = df\n",
    "    return location\n",
    "\n",
    "# Apply to both states\n",
    "FLORIDA = generate_county_level_parameters(FLORIDA, random_state=42)\n",
    "WASHINGTON = generate_county_level_parameters(WASHINGTON, random_state=42)\n",
    "\n",
    "# Quick check\n",
    "FLORIDA[\"df_sim\"][[\"COUNTY\", \"insured_properties\", \"lambda\", \"severity_mean\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida total losses (MC):\n",
      "count    1.000000e+04\n",
      "mean     8.770492e+09\n",
      "std      5.947307e+07\n",
      "min      8.559936e+09\n",
      "25%      8.730771e+09\n",
      "50%      8.770819e+09\n",
      "75%      8.809872e+09\n",
      "max      9.082146e+09\n",
      "dtype: float64\n",
      "\n",
      "Washington total losses (MC):\n",
      "count    1.000000e+04\n",
      "mean     2.133658e+09\n",
      "std      2.466344e+07\n",
      "min      2.035570e+09\n",
      "25%      2.116966e+09\n",
      "50%      2.133714e+09\n",
      "75%      2.150284e+09\n",
      "max      2.221199e+09\n",
      "dtype: float64\n",
      "\n",
      "Florida per-county mean total losses (first 5 counties):\n",
      "     COUNTY  mean_total_loss\n",
      "0   Alachua     6.533210e+07\n",
      "1     Baker     4.851068e+06\n",
      "2       Bay     1.564930e+08\n",
      "3  Bradford     7.614712e+06\n",
      "4   Brevard     3.315520e+08\n",
      "Number of MC draws: 10000\n",
      "Sample of state losses: [8.68338076e+09 8.79279320e+09 8.83236533e+09 8.86787140e+09\n",
      " 8.70840571e+09]\n",
      "Lambda per county (first 5): 0    348.418918\n",
      "1     32.597183\n",
      "2    223.988312\n",
      "3     36.494751\n",
      "4    675.239627\n",
      "Name: lambda, dtype: float64\n",
      "Severity_mean per county (first 5): 0    187458.402435\n",
      "1    148870.611203\n",
      "2    698468.183451\n",
      "3    209217.807032\n",
      "4    491055.146773\n",
      "Name: severity_mean, dtype: float64\n",
      "Relative Poisson std dev per county (first 5): 0    0.053573\n",
      "1    0.175150\n",
      "2    0.066817\n",
      "3    0.165533\n",
      "4    0.038483\n",
      "Name: lambda, dtype: float64\n",
      "County 0 losses: mean 65332102.61647634 std 3481316.8821306624\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell D – Monte Carlo simulation of county losses\n",
    "# ----------------------------\n",
    "\n",
    "def simulate_claims_mc(location, n_sim=NUM_TRIALS, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate n_sim Monte Carlo draws of total state losses:\n",
    "      - Poisson(lambda) per county\n",
    "      - Multiply by severity_mean per claim\n",
    "    Returns:\n",
    "      - state_losses: array of total state losses for n_sim draws\n",
    "      - updates location[\"df_sim\"] with per-county mean loss (optional)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = location[\"df_sim\"].copy()\n",
    "    \n",
    "    state_losses = np.zeros(n_sim)\n",
    "    \n",
    "    county_mean_losses = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        n_claims = rng.poisson(lam=row['lambda'], size=n_sim)  # shape (n_sim,)\n",
    "        county_loss = n_claims * row['severity_mean']          # shape (n_sim,)\n",
    "        state_losses += county_loss\n",
    "        \n",
    "        # store mean county loss (optional, sense check)\n",
    "        county_mean_losses.append(county_loss.mean())\n",
    "    \n",
    "    # Update df_sim with mean county loss for reference\n",
    "    df['mean_total_loss'] = county_mean_losses\n",
    "    location[\"df_sim\"] = df\n",
    "    \n",
    "    return state_losses\n",
    "\n",
    "# Run MC simulations using global NUM_TRIALS\n",
    "FL_state_losses = simulate_claims_mc(FLORIDA, n_sim=NUM_TRIALS, random_state=42)\n",
    "WA_state_losses = simulate_claims_mc(WASHINGTON, n_sim=NUM_TRIALS, random_state=42)\n",
    "\n",
    "# Quick sense check\n",
    "print(\"Florida total losses (MC):\")\n",
    "print(pd.Series(FL_state_losses).describe())\n",
    "print(\"\\nWashington total losses (MC):\")\n",
    "print(pd.Series(WA_state_losses).describe())\n",
    "\n",
    "# Optional: view per-county mean total losses\n",
    "print(\"\\nFlorida per-county mean total losses (first 5 counties):\")\n",
    "print(FLORIDA[\"df_sim\"][[\"COUNTY\", \"mean_total_loss\"]].head())\n",
    "\n",
    "#Diagnostics\n",
    "print(\"Number of MC draws:\", len(FL_state_losses))  # should be 10000\n",
    "print(\"Sample of state losses:\", FL_state_losses[:5])\n",
    "FL_df = FLORIDA['df_sim']\n",
    "print(\"Lambda per county (first 5):\", FL_df['lambda'].head(5))\n",
    "print(\"Severity_mean per county (first 5):\", FL_df['severity_mean'].head(5))\n",
    "\n",
    "# Compute Poisson relative std dev per county\n",
    "print(\"Relative Poisson std dev per county (first 5):\", np.sqrt(FL_df['lambda'].head(5)) / FL_df['lambda'].head(5))\n",
    "rng = np.random.default_rng(42)\n",
    "n_claims = rng.poisson(lam=FL_df['lambda'].iloc[0], size=10_000)\n",
    "county_losses = n_claims * FL_df['severity_mean'].iloc[0]\n",
    "print(\"County 0 losses: mean\", county_losses.mean(), \"std\", county_losses.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida sample losses (first 5 counties, 10000 draws):\n",
      "count    1.000000e+04\n",
      "mean     4.425066e+08\n",
      "std      1.519984e+07\n",
      "min      3.838450e+08\n",
      "25%      4.322039e+08\n",
      "50%      4.425425e+08\n",
      "75%      4.528414e+08\n",
      "max      4.991036e+08\n",
      "dtype: float64\n",
      "\n",
      "Washington sample losses (first 5 counties, 10000 draws):\n",
      "count    1.000000e+04\n",
      "mean     6.521152e+07\n",
      "std      3.684424e+06\n",
      "min      5.270021e+07\n",
      "25%      6.268817e+07\n",
      "50%      6.513936e+07\n",
      "75%      6.765580e+07\n",
      "max      7.950740e+07\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Cell E: Deductible & Coinsurance ---\n",
    "\n",
    "def apply_deductible_coinsurance_vectorized(county_df, deductible=10000, coinsurance=0.8, n_sim=NUM_TRIALS, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply deductible and coinsurance to county-level loss simulation.\n",
    "    Inputs:\n",
    "        county_df: DataFrame with columns ['lambda', 'severity_mean', 'insured_properties']\n",
    "        deductible: per-claim deductible\n",
    "        coinsurance: proportion insurer pays above deductible\n",
    "        n_sim: number of Monte Carlo draws per county\n",
    "    Returns:\n",
    "        state_losses: array of total state-level losses for n_sim draws\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    state_losses = np.zeros(n_sim)\n",
    "\n",
    "    for _, row in county_df.iterrows():\n",
    "        # --- simulate number of claims for this county ---\n",
    "        n_claims = rng.poisson(lam=row['lambda'], size=n_sim)\n",
    "        \n",
    "        # --- simulate individual claim severities ---\n",
    "        # assume gamma with mean=severity_mean, var=(0.5*severity_mean)^2 for variability\n",
    "        severity_shape = 4  # can be tuned\n",
    "        severity_scale = row['severity_mean'] / severity_shape\n",
    "        claim_severities = rng.gamma(shape=severity_shape, scale=severity_scale, size=(n_sim, n_claims.max()))\n",
    "        \n",
    "        # zero out excess columns if fewer claims\n",
    "        mask = np.arange(n_claims.max()) < n_claims[:, None]\n",
    "        claim_severities = claim_severities * mask\n",
    "        \n",
    "        # --- apply deductible & coinsurance ---\n",
    "        payout = np.maximum(claim_severities - deductible, 0) * coinsurance\n",
    "        \n",
    "        # sum across claims per draw\n",
    "        county_total = payout.sum(axis=1)\n",
    "        \n",
    "        # add to state total\n",
    "        state_losses += county_total\n",
    "\n",
    "    return state_losses\n",
    "\n",
    "\n",
    "# Quick sense check with first 5 counties\n",
    "FL_sim_losses = apply_deductible_coinsurance_vectorized(FLORIDA['df_sim'].head(5), n_sim=NUM_TRIALS)\n",
    "WA_sim_losses = apply_deductible_coinsurance_vectorized(WASHINGTON['df_sim'].head(5), n_sim=NUM_TRIALS)\n",
    "\n",
    "print(f\"Florida sample losses (first 5 counties, {NUM_TRIALS} draws):\")\n",
    "print(pd.Series(FL_sim_losses).describe())\n",
    "\n",
    "print(f\"\\nWashington sample losses (first 5 counties, {NUM_TRIALS} draws):\")\n",
    "print(pd.Series(WA_sim_losses).describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida metrics:\n",
      "expected_loss 442506591.7526041\n",
      "premium 531007910.1031249\n",
      "VaR_95 63676677.19751305\n",
      "TVaR_95 56972608.47487817\n",
      "profit_mean 88501318.35052085\n",
      "profit_std 15199081.138163047\n",
      "profit_bootstrap_CI (np.float64(88201673.60618936), np.float64(88796423.96507366))\n",
      "\n",
      "Washington metrics:\n",
      "expected_loss 65211516.89780401\n",
      "premium 78253820.2773648\n",
      "VaR_95 6814496.882143637\n",
      "TVaR_95 5237562.866146011\n",
      "profit_mean 13042303.37956081\n",
      "profit_std 3684240.1879088366\n",
      "profit_bootstrap_CI (np.float64(12970178.307748068), np.float64(13114509.267337078))\n"
     ]
    }
   ],
   "source": [
    "# --- Cell F: Metrics ---\n",
    "def compute_metrics(state_losses, loading=1.2, n_bootstrap=NUM_TRIALS, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute premium, profit samples, VaR, TVaR, and bootstrap CI.\n",
    "    Inputs:\n",
    "        state_losses: array of total state-level losses (after deductible & coinsurance)\n",
    "        loading: premium multiplier\n",
    "        n_bootstrap: number of bootstrap resamples for mean profit CI\n",
    "    Returns:\n",
    "        metrics: dict with expected loss, premium, VaR, TVaR, profit samples, bootstrap CI\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    state_losses = np.array(state_losses)\n",
    "    \n",
    "    # expected loss and premium\n",
    "    expected_loss = state_losses.mean()\n",
    "    premium = expected_loss * loading\n",
    "    \n",
    "    # profit samples\n",
    "    profit_samples = premium - state_losses\n",
    "    \n",
    "    # risk metrics\n",
    "    VaR_95 = np.percentile(profit_samples, 5)\n",
    "    TVaR_95 = profit_samples[profit_samples <= VaR_95].mean()\n",
    "    \n",
    "    # bootstrap CI for mean profit\n",
    "    bootstrap_means = np.array([rng.choice(profit_samples, size=len(profit_samples), replace=True).mean() for _ in range(n_bootstrap)])\n",
    "    ci_lower, ci_upper = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "    \n",
    "    metrics = {\n",
    "        'expected_loss': expected_loss,\n",
    "        'premium': premium,\n",
    "        'VaR_95': VaR_95,\n",
    "        'TVaR_95': TVaR_95,\n",
    "        'profit_mean': profit_samples.mean(),\n",
    "        'profit_std': profit_samples.std(),\n",
    "        'profit_bootstrap_CI': (ci_lower, ci_upper)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# --- Quick sense check ---\n",
    "FL_metrics = compute_metrics(FL_sim_losses)\n",
    "WA_metrics = compute_metrics(WA_sim_losses)\n",
    "\n",
    "print(\"Florida metrics:\")\n",
    "for k, v in FL_metrics.items():\n",
    "    print(k, v)\n",
    "\n",
    "print(\"\\nWashington metrics:\")\n",
    "for k, v in WA_metrics.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency GLM summary:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:     insured_properties   No. Observations:                   67\n",
      "Model:                            GLM   Df Residuals:                       65\n",
      "Model Family:                 Poisson   Df Model:                            1\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -1.3547e+05\n",
      "Date:                Thu, 11 Dec 2025   Deviance:                   2.7028e+05\n",
      "Time:                        20:02:21   Pearson chi2:                 3.09e+05\n",
      "No. Iterations:                     6   Pseudo R-squ. (CS):              1.000\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const           8.1538      0.002   3907.636      0.000       8.150       8.158\n",
      "risk_scaled     3.0207      0.004    829.566      0.000       3.014       3.028\n",
      "===============================================================================\n",
      "\n",
      "Frequency GLM gradients:\n",
      "Approximate (finite-diff): [477927.79998854 197387.02470204]\n",
      "Analytical: [1.64163794e-09 8.18545232e-10]\n",
      "Frequency GLM design matrix condition number: 5.102598455082846\n",
      "\n",
      "Severity GLM summary:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:          severity_mean   No. Observations:                   67\n",
      "Model:                            GLM   Df Residuals:                       65\n",
      "Model Family:                   Gamma   Df Model:                            1\n",
      "Link Function:                    log   Scale:                         0.25348\n",
      "Method:                          IRLS   Log-Likelihood:                -906.72\n",
      "Date:                Thu, 11 Dec 2025   Deviance:                       16.594\n",
      "Time:                        20:02:21   Pearson chi2:                     16.5\n",
      "No. Iterations:                    10   Pseudo R-squ. (CS):          0.0001240\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          12.9109      0.073    177.255      0.000      12.768      13.054\n",
      "risk_scaled     0.0293      0.309      0.095      0.924      -0.576       0.634\n",
      "===============================================================================\n",
      "\n",
      "Severity GLM gradients:\n",
      "Approximate (finite-diff): [27216776.14692598  3471460.817568  ]\n",
      "Analytical: [-2.34413561e-10  1.58962635e-08]\n",
      "Severity GLM design matrix condition number: 5.102598455082846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell G – Diagnostics\n",
    "# ----------------------------\n",
    "\n",
    "epsilon = 1e-5  # for finite-difference gradient\n",
    "\n",
    "# --- Frequency GLM (Poisson) ---\n",
    "df = FLORIDA['df_glm'].copy()\n",
    "y_freq = df['insured_properties']\n",
    "X_freq = sm.add_constant(df[['risk_scaled']])  # example covariate\n",
    "\n",
    "freq_model = sm.GLM(y_freq, X_freq, family=sm.families.Poisson())\n",
    "freq_results = freq_model.fit()\n",
    "print(\"Frequency GLM summary:\")\n",
    "print(freq_results.summary())\n",
    "\n",
    "# Finite-difference gradient check\n",
    "beta = freq_results.params.values\n",
    "grad_approx = np.zeros_like(beta)\n",
    "for i in range(len(beta)):\n",
    "    beta_plus = beta.copy(); beta_plus[i] += epsilon\n",
    "    beta_minus = beta.copy(); beta_minus[i] -= epsilon\n",
    "    mu_plus = np.exp(X_freq @ beta_plus)\n",
    "    mu_minus = np.exp(X_freq @ beta_minus)\n",
    "    grad_approx[i] = (mu_plus.sum() - mu_minus.sum()) / (2*epsilon)\n",
    "\n",
    "# Analytical gradient (score function)\n",
    "mu = freq_results.fittedvalues\n",
    "grad_analytical = X_freq.values.T @ (y_freq.values - mu)\n",
    "\n",
    "print(\"\\nFrequency GLM gradients:\")\n",
    "print(\"Approximate (finite-diff):\", grad_approx)\n",
    "print(\"Analytical:\", grad_analytical)\n",
    "\n",
    "# Condition number\n",
    "X_freq_cond = cond(X_freq.values)\n",
    "print(\"Frequency GLM design matrix condition number:\", X_freq_cond)\n",
    "\n",
    "# --- Severity GLM (Gamma, log link) ---\n",
    "y_sev = df['severity_mean']\n",
    "X_sev = sm.add_constant(df[['risk_scaled']])  # same covariate\n",
    "\n",
    "sev_model = sm.GLM(y_sev, X_sev, family=sm.families.Gamma(sm.families.links.log()))\n",
    "sev_results = sev_model.fit()\n",
    "print(\"\\nSeverity GLM summary:\")\n",
    "print(sev_results.summary())\n",
    "\n",
    "# Finite-difference gradient check\n",
    "beta_sev = sev_results.params.values\n",
    "grad_approx_sev = np.zeros_like(beta_sev)\n",
    "for i in range(len(beta_sev)):\n",
    "    beta_plus = beta_sev.copy(); beta_plus[i] += epsilon\n",
    "    beta_minus = beta_sev.copy(); beta_minus[i] -= epsilon\n",
    "    mu_plus = np.exp(X_sev @ beta_plus)\n",
    "    mu_minus = np.exp(X_sev @ beta_minus)\n",
    "    grad_approx_sev[i] = (mu_plus.sum() - mu_minus.sum()) / (2*epsilon)\n",
    "\n",
    "# Analytical gradient (simplified)\n",
    "mu_sev = sev_results.fittedvalues\n",
    "grad_analytical_sev = X_sev.values.T @ ((y_sev.values - mu_sev) / (mu_sev ** 2))\n",
    "\n",
    "print(\"\\nSeverity GLM gradients:\")\n",
    "print(\"Approximate (finite-diff):\", grad_approx_sev)\n",
    "print(\"Analytical:\", grad_analytical_sev)\n",
    "\n",
    "# Condition number\n",
    "X_sev_cond = cond(X_sev.values)\n",
    "print(\"Severity GLM design matrix condition number:\", X_sev_cond)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
