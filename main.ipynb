{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MARKET_SHARE = 0.05 # from \n",
    "AVERAGE_HOUSEHOLD_SIZE = 2.5 # US average according to ...\n",
    "# recommended default baseline (actuarial starting point). Tune / replace with real data if available.\n",
    "BASE_RATE = 1 / 18  # US average annual claim probability per property (sensible starting point)\n",
    "NUM_TRIALS = 10000  # Monte Carlo draws per county/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import gamma\n",
    "from scipy.integrate import quad\n",
    "from numpy.linalg import cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counties for Florida:\n",
      "     COUNTY  insured_properties  lambda_base  severity_mean\n",
      "0   Alachua         6177.422222   343.190123  196545.729472\n",
      "1     Baker          618.933333    34.385185  137107.148543\n",
      "2       Bay         3885.977778   215.887654  712090.788201\n",
      "3  Bradford          627.400000    34.855556  239810.046710\n",
      "4   Brevard        13468.155556   748.230864  553818.713457\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell A – Data Loading + Preprocessing\n",
    "# ----------------------------\n",
    "\n",
    "# Relevant property information\n",
    "FLORIDA = {\n",
    "    \"name\": \"Florida\",\n",
    "    \"average_property_cost\": 325000, \n",
    "}\n",
    "WASHINGTON = {\n",
    "    \"name\": \"Washington\",\n",
    "    \"average_property_cost\": 610000, \n",
    "}\n",
    "\n",
    "# Load FEMA CSVs\n",
    "FLORIDA['df'] = pd.read_csv('fl_county.csv')\n",
    "WASHINGTON['df'] = pd.read_csv('wa_county.csv')\n",
    "\n",
    "# ----------------------------\n",
    "# CLEAN + PREPARE FEMA DATA\n",
    "# ----------------------------\n",
    "def clean_fema_df(state_dict):\n",
    "    df = state_dict[\"df\"].copy()\n",
    "\n",
    "    # Keep only the necessary columns\n",
    "    needed = [\"COUNTY\", \"POPULATION\", \"BUILDVALUE\", \"EAL_VALT\", \"RISK_VALUE\", \"RISK_SCORE\"]\n",
    "    df = df[needed]\n",
    "\n",
    "    # Rename for clarity\n",
    "    df = df.rename(columns={\n",
    "        \"BUILDVALUE\": \"exposure\",  # total building value\n",
    "        \"EAL_VALT\": \"EAL\",         # total expected annual loss\n",
    "        \"POPULATION\": \"population\"\n",
    "    })\n",
    "\n",
    "    # Convert numeric\n",
    "    df[\"exposure\"] = pd.to_numeric(df[\"exposure\"], errors=\"coerce\")\n",
    "    df[\"EAL\"] = pd.to_numeric(df[\"EAL\"], errors=\"coerce\")\n",
    "    df[\"population\"] = pd.to_numeric(df[\"population\"], errors=\"coerce\")\n",
    "    df[\"RISK_VALUE\"] = pd.to_numeric(df[\"RISK_VALUE\"], errors=\"coerce\")\n",
    "    df[\"RISK_SCORE\"] = pd.to_numeric(df[\"RISK_SCORE\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with missing exposure or EAL\n",
    "    df = df.dropna(subset=[\"exposure\", \"EAL\"])\n",
    "\n",
    "    state_dict[\"df_clean\"] = df\n",
    "    return state_dict\n",
    "\n",
    "FLORIDA = clean_fema_df(FLORIDA)\n",
    "WASHINGTON = clean_fema_df(WASHINGTON)\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocessing for GLM / Monte Carlo\n",
    "# ----------------------------\n",
    "\n",
    "def preprocess_location_df(location):\n",
    "    \"\"\"\n",
    "    Produces county-level variables for frequency × severity modeling:\n",
    "    - insured_properties: population / average household size\n",
    "    - lambda_base: expected claims per county based on empirical claim rate (~1/18)\n",
    "    - severity_mean: EAL per expected claim\n",
    "    - risk_scaled: normalized FEMA RISK_VALUE\n",
    "    \"\"\"\n",
    "    df = location[\"df_clean\"].copy()\n",
    "\n",
    "    # Estimate # of properties\n",
    "    df[\"properties\"] = df[\"population\"] / AVERAGE_HOUSEHOLD_SIZE\n",
    "\n",
    "    # Assume ~1/18 of properties claim per year\n",
    "    base_claim_rate = 1/18\n",
    "    df[\"insured_properties\"] = df[\"properties\"] * base_claim_rate\n",
    "\n",
    "    # baseline lambda = insured properties * empirical base rate\n",
    "    df[\"lambda_base\"] = df[\"insured_properties\"] * BASE_RATE\n",
    "\n",
    "\n",
    "    # severity per claim\n",
    "    df[\"severity_mean\"] = df[\"EAL\"] / df[\"lambda_base\"]\n",
    "\n",
    "    # normalized risk factor\n",
    "    df[\"risk_scaled\"] = df[\"RISK_VALUE\"] / df[\"RISK_VALUE\"].max()\n",
    "\n",
    "    location[\"df_glm\"] = df\n",
    "    return df\n",
    "\n",
    "FLORIDA[\"df_glm\"] = preprocess_location_df(FLORIDA)\n",
    "WASHINGTON[\"df_glm\"] = preprocess_location_df(WASHINGTON)\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Sample counties for Florida:\")\n",
    "print(FLORIDA[\"df_glm\"][[\"COUNTY\", \"insured_properties\", \"lambda_base\", \"severity_mean\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>insured_properties</th>\n",
       "      <th>lambda</th>\n",
       "      <th>severity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alachua</td>\n",
       "      <td>6177.422222</td>\n",
       "      <td>348.418918</td>\n",
       "      <td>187458.402435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baker</td>\n",
       "      <td>618.933333</td>\n",
       "      <td>32.597183</td>\n",
       "      <td>148870.611203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bay</td>\n",
       "      <td>3885.977778</td>\n",
       "      <td>223.988312</td>\n",
       "      <td>698468.183451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bradford</td>\n",
       "      <td>627.400000</td>\n",
       "      <td>36.494751</td>\n",
       "      <td>209217.807032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brevard</td>\n",
       "      <td>13468.155556</td>\n",
       "      <td>675.239627</td>\n",
       "      <td>491055.146773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     COUNTY  insured_properties      lambda  severity_mean\n",
       "0   Alachua         6177.422222  348.418918  187458.402435\n",
       "1     Baker          618.933333   32.597183  148870.611203\n",
       "2       Bay         3885.977778  223.988312  698468.183451\n",
       "3  Bradford          627.400000   36.494751  209217.807032\n",
       "4   Brevard        13468.155556  675.239627  491055.146773"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell B\n",
    "\n",
    "def generate_county_level_parameters(location, perturb_std_lambda=0.05, perturb_std_severity=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Produce county-level stochastic frequency (lambda) and severity.\n",
    "    \n",
    "    Args:\n",
    "        location: dict containing df_clean / df_glm\n",
    "        perturb_std_lambda: relative std dev for lambda perturbation (5% default)\n",
    "        perturb_std_severity: relative std dev for severity perturbation (10% default)\n",
    "        random_state: seed for reproducibility\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = location[\"df_glm\"].copy()\n",
    "    \n",
    "    # --- Perturb lambda around base ---\n",
    "    df[\"lambda\"] = df[\"lambda_base\"] * (1 + rng.normal(0, perturb_std_lambda, size=len(df)))\n",
    "    df[\"lambda\"] = df[\"lambda\"].clip(lower=0.01)  # prevent negative lambda\n",
    "\n",
    "    # --- Perturb severity around mean ---\n",
    "    df[\"severity_mean\"] = df[\"severity_mean\"] * (1 + rng.normal(0, perturb_std_severity, size=len(df)))\n",
    "    df[\"severity_mean\"] = df[\"severity_mean\"].clip(lower=1000)  # prevent absurdly low severity\n",
    "\n",
    "    \n",
    "    location[\"df_sim\"] = df\n",
    "    return location\n",
    "\n",
    "# Apply to both states\n",
    "FLORIDA = generate_county_level_parameters(FLORIDA, random_state=42)\n",
    "WASHINGTON = generate_county_level_parameters(WASHINGTON, random_state=42)\n",
    "\n",
    "# Quick check\n",
    "FLORIDA[\"df_sim\"][[\"COUNTY\", \"insured_properties\", \"lambda\", \"severity_mean\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida sample losses (first 5 counties, 10000 draws):\n",
      "count    1.000000e+04\n",
      "mean     4.419479e+08\n",
      "std      1.335672e+07\n",
      "min      3.868385e+08\n",
      "25%      4.328650e+08\n",
      "50%      4.417228e+08\n",
      "75%      4.510809e+08\n",
      "max      4.886637e+08\n",
      "dtype: float64\n",
      "\n",
      "Washington sample losses (first 5 counties, 10000 draws):\n",
      "count    1.000000e+04\n",
      "mean     6.513420e+07\n",
      "std      3.212604e+06\n",
      "min      5.341716e+07\n",
      "25%      6.300491e+07\n",
      "50%      6.511345e+07\n",
      "75%      6.724170e+07\n",
      "max      7.852265e+07\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Cell D: Monte Carlo Simulation with Quadrature for Severity ---\n",
    "\n",
    "from scipy.stats import gamma\n",
    "import numpy as np\n",
    "\n",
    "def expected_payout_quadrature(deductible, coinsurance, shape, scale, n=200):\n",
    "    \"\"\"\n",
    "    Computes E[(X - deductible)^+] * coinsurance for Gamma(shape, scale)\n",
    "    using n-point Gauss–Legendre quadrature.\n",
    "    \"\"\"\n",
    "    # Upper limit chosen as 99.99th percentile of severity distribution\n",
    "    upper = gamma.ppf(0.9999, a=shape, scale=scale)\n",
    "    \n",
    "    # Gauss–Legendre nodes & weights on [-1,1]\n",
    "    xs, ws = np.polynomial.legendre.leggauss(n)\n",
    "    \n",
    "    # Transform to [deductible, upper]\n",
    "    t = 0.5 * (xs + 1) * (upper - deductible) + deductible\n",
    "    w = 0.5 * (upper - deductible) * ws\n",
    "\n",
    "    # Integrand: (x - d) * f_X(x)\n",
    "    integrand = (t - deductible) * gamma.pdf(t, a=shape, scale=scale)\n",
    "    \n",
    "    # Multiply by coinsurance factor\n",
    "    return coinsurance * np.sum(w * integrand)\n",
    "\n",
    "\n",
    "def apply_deductible_coinsurance_vectorized(county_df, deductible=10000, coinsurance=0.8,\n",
    "                                            n_sim=NUM_TRIALS, random_state=42):\n",
    "    \"\"\"\n",
    "    Monte Carlo simulation of state losses:\n",
    "        - Poisson frequency simulation\n",
    "        - Expected severity payout computed via Gauss–Legendre quadrature\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    state_losses = np.zeros(n_sim)\n",
    "\n",
    "    for _, row in county_df.iterrows():\n",
    "        \n",
    "        # --- 1. Simulate claim frequency for this county ---\n",
    "        n_claims = rng.poisson(lam=row['lambda'], size=n_sim)\n",
    "\n",
    "        # --- 2. Quadrature: expected payout per claim ---\n",
    "        severity_shape = 4  # can be tuned\n",
    "        severity_scale = row['severity_mean'] / severity_shape\n",
    "\n",
    "        E_payout = expected_payout_quadrature(\n",
    "            deductible=deductible,\n",
    "            coinsurance=coinsurance,\n",
    "            shape=severity_shape,\n",
    "            scale=severity_scale\n",
    "        )\n",
    "\n",
    "        # --- 3. Total losses per simulation draw ---\n",
    "        county_total = n_claims * E_payout\n",
    "\n",
    "        # accumulate into state-level losses\n",
    "        state_losses += county_total\n",
    "\n",
    "    return state_losses\n",
    "\n",
    "\n",
    "# Quick sense check with first 5 counties\n",
    "FL_sim_losses = apply_deductible_coinsurance_vectorized(FLORIDA['df_sim'].head(5), n_sim=NUM_TRIALS)\n",
    "WA_sim_losses = apply_deductible_coinsurance_vectorized(WASHINGTON['df_sim'].head(5), n_sim=NUM_TRIALS)\n",
    "\n",
    "print(f\"Florida sample losses (first 5 counties, {NUM_TRIALS} draws):\")\n",
    "print(pd.Series(FL_sim_losses).describe())\n",
    "\n",
    "print(f\"\\nWashington sample losses (first 5 counties, {NUM_TRIALS} draws):\")\n",
    "print(pd.Series(WA_sim_losses).describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida metrics:\n",
      "expected_loss 441947927.29505956\n",
      "premium 530337512.7540715\n",
      "VaR_95 66432449.776738405\n",
      "TVaR_95 60847769.847131684\n",
      "profit_mean 88389585.45901182\n",
      "profit_std 13356048.414985042\n",
      "profit_bootstrap_CI (np.float64(88125034.92020176), np.float64(88652989.35282354))\n",
      "\n",
      "Washington metrics:\n",
      "expected_loss 65134201.33090066\n",
      "premium 78161041.5970808\n",
      "VaR_95 7751648.710351333\n",
      "TVaR_95 6288408.535639963\n",
      "profit_mean 13026840.266180133\n",
      "profit_std 3212442.9793348312\n",
      "profit_bootstrap_CI (np.float64(12962766.545188608), np.float64(13090408.111911109))\n"
     ]
    }
   ],
   "source": [
    "# --- Cell E: Metrics ---\n",
    "def compute_metrics(state_losses, loading=1.2, n_bootstrap=NUM_TRIALS, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute premium, profit samples, VaR, TVaR, and bootstrap CI.\n",
    "    Inputs:\n",
    "        state_losses: array of total state-level losses (after deductible & coinsurance)\n",
    "        loading: premium multiplier\n",
    "        n_bootstrap: number of bootstrap resamples for mean profit CI\n",
    "    Returns:\n",
    "        metrics: dict with expected loss, premium, VaR, TVaR, profit samples, bootstrap CI\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    state_losses = np.array(state_losses)\n",
    "    \n",
    "    # expected loss and premium\n",
    "    expected_loss = state_losses.mean()\n",
    "    premium = expected_loss * loading\n",
    "    \n",
    "    # profit samples\n",
    "    profit_samples = premium - state_losses\n",
    "    \n",
    "    # risk metrics\n",
    "    VaR_95 = np.percentile(profit_samples, 5)\n",
    "    TVaR_95 = profit_samples[profit_samples <= VaR_95].mean()\n",
    "    \n",
    "    # bootstrap CI for mean profit\n",
    "    bootstrap_means = np.array([rng.choice(profit_samples, size=len(profit_samples), replace=True).mean() for _ in range(n_bootstrap)])\n",
    "    ci_lower, ci_upper = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "    \n",
    "    metrics = {\n",
    "        'expected_loss': expected_loss,\n",
    "        'premium': premium,\n",
    "        'VaR_95': VaR_95,\n",
    "        'TVaR_95': TVaR_95,\n",
    "        'profit_mean': profit_samples.mean(),\n",
    "        'profit_std': profit_samples.std(),\n",
    "        'profit_bootstrap_CI': (ci_lower, ci_upper)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# --- Quick sense check ---\n",
    "FL_metrics = compute_metrics(FL_sim_losses)\n",
    "WA_metrics = compute_metrics(WA_sim_losses)\n",
    "\n",
    "print(\"Florida metrics:\")\n",
    "for k, v in FL_metrics.items():\n",
    "    print(k, v)\n",
    "\n",
    "print(\"\\nWashington metrics:\")\n",
    "for k, v in WA_metrics.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency GLM summary:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:     insured_properties   No. Observations:                   67\n",
      "Model:                            GLM   Df Residuals:                       65\n",
      "Model Family:                 Poisson   Df Model:                            1\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -1.3547e+05\n",
      "Date:                Fri, 12 Dec 2025   Deviance:                   2.7028e+05\n",
      "Time:                        00:08:17   Pearson chi2:                 3.09e+05\n",
      "No. Iterations:                     6   Pseudo R-squ. (CS):              1.000\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const           8.1538      0.002   3907.636      0.000       8.150       8.158\n",
      "risk_scaled     3.0207      0.004    829.566      0.000       3.014       3.028\n",
      "===============================================================================\n",
      "\n",
      "Frequency GLM gradients:\n",
      "Approximate (finite-diff): [477927.79998854 197387.02470204]\n",
      "Analytical: [1.64163794e-09 8.18545232e-10]\n",
      "Frequency GLM design matrix condition number: 5.102598455082846\n",
      "\n",
      "Severity GLM summary:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:          severity_mean   No. Observations:                   67\n",
      "Model:                            GLM   Df Residuals:                       65\n",
      "Model Family:                   Gamma   Df Model:                            1\n",
      "Link Function:                    log   Scale:                         0.25348\n",
      "Method:                          IRLS   Log-Likelihood:                -906.72\n",
      "Date:                Fri, 12 Dec 2025   Deviance:                       16.594\n",
      "Time:                        00:08:17   Pearson chi2:                     16.5\n",
      "No. Iterations:                    10   Pseudo R-squ. (CS):          0.0001240\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          12.9109      0.073    177.255      0.000      12.768      13.054\n",
      "risk_scaled     0.0293      0.309      0.095      0.924      -0.576       0.634\n",
      "===============================================================================\n",
      "\n",
      "Severity GLM gradients:\n",
      "Approximate (finite-diff): [27216776.14692598  3471460.817568  ]\n",
      "Analytical: [-2.34413561e-10  1.58962635e-08]\n",
      "Severity GLM design matrix condition number: 5.102598455082846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell F – Diagnostics\n",
    "# ----------------------------\n",
    "\n",
    "epsilon = 1e-5  # for finite-difference gradient\n",
    "\n",
    "# --- Frequency GLM (Poisson) ---\n",
    "df = FLORIDA['df_glm'].copy()\n",
    "y_freq = df['insured_properties']\n",
    "X_freq = sm.add_constant(df[['risk_scaled']])  # example covariate\n",
    "\n",
    "freq_model = sm.GLM(y_freq, X_freq, family=sm.families.Poisson())\n",
    "freq_results = freq_model.fit()\n",
    "print(\"Frequency GLM summary:\")\n",
    "print(freq_results.summary())\n",
    "\n",
    "# Finite-difference gradient check\n",
    "beta = freq_results.params.values\n",
    "grad_approx = np.zeros_like(beta)\n",
    "for i in range(len(beta)):\n",
    "    beta_plus = beta.copy(); beta_plus[i] += epsilon\n",
    "    beta_minus = beta.copy(); beta_minus[i] -= epsilon\n",
    "    mu_plus = np.exp(X_freq @ beta_plus)\n",
    "    mu_minus = np.exp(X_freq @ beta_minus)\n",
    "    grad_approx[i] = (mu_plus.sum() - mu_minus.sum()) / (2*epsilon)\n",
    "\n",
    "# Analytical gradient (score function)\n",
    "mu = freq_results.fittedvalues\n",
    "grad_analytical = X_freq.values.T @ (y_freq.values - mu)\n",
    "\n",
    "print(\"\\nFrequency GLM gradients:\")\n",
    "print(\"Approximate (finite-diff):\", grad_approx)\n",
    "print(\"Analytical:\", grad_analytical)\n",
    "\n",
    "# Condition number\n",
    "X_freq_cond = cond(X_freq.values)\n",
    "print(\"Frequency GLM design matrix condition number:\", X_freq_cond)\n",
    "\n",
    "# --- Severity GLM (Gamma, log link) ---\n",
    "y_sev = df['severity_mean']\n",
    "X_sev = sm.add_constant(df[['risk_scaled']])  # same covariate\n",
    "\n",
    "sev_model = sm.GLM(y_sev, X_sev, family=sm.families.Gamma(sm.families.links.log()))\n",
    "sev_results = sev_model.fit()\n",
    "print(\"\\nSeverity GLM summary:\")\n",
    "print(sev_results.summary())\n",
    "\n",
    "# Finite-difference gradient check\n",
    "beta_sev = sev_results.params.values\n",
    "grad_approx_sev = np.zeros_like(beta_sev)\n",
    "for i in range(len(beta_sev)):\n",
    "    beta_plus = beta_sev.copy(); beta_plus[i] += epsilon\n",
    "    beta_minus = beta_sev.copy(); beta_minus[i] -= epsilon\n",
    "    mu_plus = np.exp(X_sev @ beta_plus)\n",
    "    mu_minus = np.exp(X_sev @ beta_minus)\n",
    "    grad_approx_sev[i] = (mu_plus.sum() - mu_minus.sum()) / (2*epsilon)\n",
    "\n",
    "# Analytical gradient (simplified)\n",
    "mu_sev = sev_results.fittedvalues\n",
    "grad_analytical_sev = X_sev.values.T @ ((y_sev.values - mu_sev) / (mu_sev ** 2))\n",
    "\n",
    "print(\"\\nSeverity GLM gradients:\")\n",
    "print(\"Approximate (finite-diff):\", grad_approx_sev)\n",
    "print(\"Analytical:\", grad_analytical_sev)\n",
    "\n",
    "# Condition number\n",
    "X_sev_cond = cond(X_sev.values)\n",
    "print(\"Severity GLM design matrix condition number:\", X_sev_cond)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
