{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as any collaborators you worked with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To receive credit for this assignment, you must also fill out the [AI Use survey](https://forms.gle/ZhR5k8TdAeN8rj4CA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 16\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d32b4bf706b10b4d167573f34b78f1b7",
     "grade": false,
     "grade_id": "cell-c4123d20c3a14019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Final Project\n",
    "\n",
    "This notebook will provide a brief structure and rubric for presenting your final project. \n",
    "\n",
    "The purpose of the project is 2-fold\n",
    "* To give you an opportunity to work on a problem you are truly interested in (as this is the best way to actually learn something)\n",
    "* To demonstrate to me that you understand the overall workflow of problem solving from problem selection to implementation to discussion \n",
    "\n",
    "You can choose any subject area that interests you as long as there is a computational component to it.  However, please do not reuse projects or homeworks you have done in other classes.  This should be **your** original work.\n",
    "\n",
    "**You can work in teams, but clearly identify each persons contribution** and every team member should hand in their own copy of the notebook.\n",
    "\n",
    "### Structure\n",
    "There are 5 parts for a total of 100 points that provide the overall structure of a mini research project.\n",
    "\n",
    "* Abstract\n",
    "* Introduction and Problem Description\n",
    "* Brief discussion of Computational approach and import of any additional packages\n",
    "* Implementation including tests\n",
    "* Discussion of results and future directions\n",
    "\n",
    "For grading purposes, please try to make this notebook entirely self contained. \n",
    "\n",
    "The project is worth about 2 problem sets and should be of comparable length (please: I will have about 100 of these to read and I am not expecting full 10 page papers).  The actual project does not necessarily have to work but in that case you should demonstrate that you understand why it did not work and what steps you would take next to fix it.\n",
    "\n",
    "Have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbc13653b2b30717b82998dc8185d077",
     "grade": false,
     "grade_id": "cell-382e921cda790b91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Abstract [10 pts]\n",
    "\n",
    "Provide a 1-2 paragraph abstract of the project in the style of a research paper.  The abstract should contain\n",
    "\n",
    "* A brief description of the problem\n",
    "* A brief justification describing why this problem is important/interesting to you\n",
    "* A general description of the computational approach\n",
    "* A brief summary of what you did and what you learned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a13e715b5cd07bdd5e0f62ba3ffd3b70",
     "grade": true,
     "grade_id": "cell-2152ede583bdfd2e",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Every year, extreme weather events cause damage to homes across the United States, many of which are insured. For a property insurance carrier, it is important to model the risks and expected losses that arise from underwriting home insurance policies, especially in states that experience a high frequency of extreme weather events. Thus, this project aims to use FEMA data to model the claims, expected losses, and profits that an insurer might face in states like Florida and Washington. I am personally interested in exploring this problem because I am an aspiring actuary and would like to explore Catastrophe Modeling as applied to the property insurance space, which is full of complex problems and real-world implications.\n",
    "\n",
    "I began by collecting data from FEMA's National Risk Index, which provides county-level metrics like proprietary natural hazard risk scores and Expected Annual Loss (EAL). Taking the EAL and other assumptions about the insurer- such as market share, baseline claim rate, and average insured home value- Generalized Linear Models (GLMs) are fitted to estimate frequency and severity parameters for the underlying claim distributions. Monte Carlo simulation and numerical quadrature is used to capture typical insurance policy mechanics like deductibles and coinsurance rates, which factor into estimating the expected profit for the insurer in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a85e6fdaa77e6624fee7a062c4fbae2",
     "grade": false,
     "grade_id": "cell-318b53f4ed873060",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction [15 pts]\n",
    "\n",
    "In ~4-5 paragraphs, describe \n",
    "* The general problem you want to solve\n",
    "* Why it is important and what you hope to achieve.\n",
    "\n",
    "Please provide basic **references**, particularly if you are reproducing results from a paper. Also include any basic equations you plan to solve. \n",
    "\n",
    "Please use proper spelling and grammar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "389f9029792ef9416cb9a4531f47cfa0",
     "grade": true,
     "grade_id": "cell-f3a6a5fa3a14f053",
     "locked": false,
     "points": 13,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Property insurance carriers are interested in understanding the risks, as well as the payoffs and potential profits involved in underwriting policies (i.e. providing insurance contracts) in a given market. This involves modeling the expected losses and premiums that the insurer could face, as well as modeling tail-end outcomes that correspond to a year with a high frequency and/or severity of extreme weather events that trigger claims (i.e. liabilities for the insurer) to be filed. These model considerations are especially important for insurers underwriting policies in states that experience relatively many extreme weather events, like Florida and Washington.\n",
    "\n",
    "Florida is notorious for its exposure to hurricanes, while Washington is a geographically diverse state that normally experiences torrential rainfall, flooding, wildfires, and winter storms. Overall, climate is one of the most important considerations when creating insurance models that forecast losses and profits in any state. Climate change and new regulations continually affect the property insurance indsutry, making the problem of financial forecasting a dynamic and crucial one for insurers. To illustrate, California passed new legislation this Summer which allows insurance companies to develop Wildfire Catastrophe Models to aid their fire insurance underwriting, whereas beforehand, most companies chose to stay out of California by default because of the high-level of risk that they were not allowed to properly estimate.\n",
    "\n",
    "Furthermore, the impact of this forecasting problem extends far beyond just predicting a hypothetical insurance company's bottom line– the financial well-beings of homeowners are also at stake when an insurer develops such a model, as it helps them design the terms of insurance policies in a given area. However, this project is not a full-fledged catastrophe model that considers several types of perils (e.g. fire, flood). This project is an end-to-end model that takes FEMA National Risk Index data, which is publicly available on a county level, to fit statistical models and run Monte Carlo simulations to forecast claims in the states of Florida and Washington. Also, numerical integration is used to solve one form of a classical profit equation in insurance, as follows.\n",
    "\n",
    "The Gauss-Legendre quadrature method is used to approximate the insurer's expected payout per claim, which is modeled by\n",
    "\n",
    "$$E[(X-d)^+]=\\int_d^{\\infty} (x-d)f_X(x)dx,$$\n",
    "\n",
    "where the claim severity $X$ follows a Gamma distribution, $d$ is the deductible, and $c$ is the coinsruance rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73f86182652b4056f26270c1819c32b4",
     "grade": false,
     "grade_id": "cell-cf3ab00000465969",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0da0870c2fa4c6e72b6e6b9b3485b47e",
     "grade": true,
     "grade_id": "cell-a11d878ca3210c53",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[1] Federal Emergency Management Agency. (n.d.). National risk index: Data & resources. FEMA.\n",
    "https://hazards.fema.gov/nri/data-resources   \n",
    "\n",
    "[2] Insurance Information Institute. (2025). Homeowners insurance claims frequency: About one in 18 insured homes has a claim. Retrieved from https://www.iii.org/fact-statistic/facts-statistics-homeowners-and-renters-insurance\n",
    "\n",
    "[3] USAFacts. (2023). Why are US homes getting bigger while households shrink? Retrieved from https://usafacts.org/data/topics/people‑society/population‑and‑demographics/population‑data/average‑family‑size/\n",
    "\n",
    "[4] Fu, L. (2004). Severity distributions for GLMs: Gamma or lognormal? Casualty Actuarial Society. Gamma distributions are commonly used for modeling claim severity due to their support on positive values and ability to capture right‑skewed behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3643243954ea23989f077101458711e3",
     "grade": false,
     "grade_id": "cell-a7a4255dfbbc98e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Computational  Methods [10 pts]\n",
    "\n",
    "Describe the specific approach you will take to solve some concrete aspect of the general problem. \n",
    "\n",
    "You should  include all the numerical or computational methods you intend to use.  These can include methods or packages  we did not discuss in class but provide some reference to the method. You do not need to explain in detail how the methods work, but you should describe their basic functionality and justify your choices. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36d5c668364853007ee40c22680478d2",
     "grade": true,
     "grade_id": "cell-fe71c0040eae7d5d",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The project models insured losses and profits for a small insurer in the states of Florida and Washington by combining a traditional frequency-severity statistical model with stochastic Monte Carlo simulations and deterministic numerical integration. There are 3 computational stages in the predictive model pipeline:\n",
    "\n",
    "1. Parameter estimation and perturbation for underlying claim distributions\n",
    "The FEMA data provides county-level data that is fitted by two complementary Generalized Linear Models (GLMs) for the classic Frequency-Severity approach. The Frequency GLM models claim counts with a Poisson distribution , and the Severity GLM models individual claim costs with a Gamma distribution. Breaking down losses into the frequency and severity of claims is natural, as the total cost of the claims an insurer can expect to be liable for is total number of claims * expected cost of claims. Both GLMs are estimated using Iteratively Reweighted Least Squares (IRLS) with the statsmodels Python package. To account for heterogeneity between counties in a state, the fitted parameters for each GLM are stochastically perturbed by Gaussian noise.\n",
    "\n",
    "2. Monte Carlo and Numerical Integration for simulating claims and profits\n",
    "Claim counts are simulated by a Poisson distribution with the perturbed parameter for each county. The justification for the numerical integration is explained in a later markdown cell in the implementation.\n",
    "\n",
    "3. Compute financial outcomes and Bootstrap Confidence Intervals\n",
    "State-level premiums and profit calculations are arithmetic about the expected losses that the model predicts. From the empirical distribution of profits that are simulated, Value at Risk and Tail Value at Risk are calculated to understand the risk surrounding profits in each state better. The uncertainty in the expected profit is captured by nonparametric bootstrap resampling, which yields a 95% confidence interval without assuming normality in the empirical distribution of simulated profits.\n",
    "\n",
    "In summary, this end-to-end approach to an insurance model is described by the following pipeline of fitting GLMs, stochastically perturbing parameters, running Monte Carlo simulations and approximations with numerical integration, and computing profit and risk metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9bfbac8a8b59723bad5399bef5ac3a9",
     "grade": false,
     "grade_id": "cell-92a6ea193d5ce960",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**If you need to install or import any additional python packages,  please provide complete installation instructions in the code block below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide complete installation or import information for external packages or modules here e.g.\n",
    "\n",
    "# the following packages are included in the Anaconda distribution\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import gamma\n",
    "from numpy.linalg import cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f44617596502a662582b72d9de28f96",
     "grade": false,
     "grade_id": "cell-501a9781d3f83013",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Implementation [50 pts]\n",
    "\n",
    "Use the Markdown and Code blocks below to implement and document your methods including figures.  Only the first markdown block will be a grading cell but please add (not copy) cells in this section to organize your work. \n",
    "\n",
    "Please make the description of your problem readable by interlacing clear explanatory text with code (again with proper grammar and spelling). \n",
    "All code should be well described and commented.\n",
    "\n",
    "For at least one routine you code below, you should provide a test block (e.g. using `numpy.testing` routines, or a convergence plot) to validate your code.  \n",
    "\n",
    "An **important** component of any computational paper is to demonstrate to yourself and others that your code is producing correct results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b30ba16024abd06acd9908c692423f6d",
     "grade": true,
     "grade_id": "cell-31f08d5d85bd9afd",
     "locked": false,
     "points": 50,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The implementation is broken down into 5 code cells besides the global variable cell:\n",
    "\n",
    "1. Loading and preprocessing the data\n",
    "\n",
    "2. The three computational steps of the model pipeline described in the previous section\n",
    "\n",
    "3. Numerical iagnostics to assess the success of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "MARKET_SHARE = 0.05 # percent of households in a state that the insurer covers\n",
    "AVERAGE_HOUSEHOLD_SIZE = 2.5 # US average (reference 3)\n",
    "BASE_RATE = 1 / 18  # US average annual claim probability per property (reference 2)\n",
    "NUM_TRIALS = 10000  # Monte Carlo draws per county/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Initialize states\n",
    "FLORIDA = {\n",
    "    \"name\": \"Florida\",\n",
    "}\n",
    "WASHINGTON = {\n",
    "    \"name\": \"Washington\",\n",
    "}\n",
    "\n",
    "# Load + clean FEMA data\n",
    "def load_and_clean_fema(file_path):\n",
    "    df = pd.read_csv(file_path)[\n",
    "        [\"COUNTY\", \"POPULATION\", \"BUILDVALUE\", \"EAL_VALT\", \"RISK_VALUE\", \"RISK_SCORE\"] # keep only the fields that are used in the model\n",
    "    ]\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"BUILDVALUE\": \"exposure\",\n",
    "        \"EAL_VALT\": \"EAL\",\n",
    "        \"POPULATION\": \"population\"\n",
    "    })\n",
    "\n",
    "    numeric_cols = [\"exposure\", \"EAL\", \"population\", \"RISK_VALUE\", \"RISK_SCORE\"]\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    return df.dropna(subset=[\"exposure\", \"EAL\"]) # ensure no NA values\n",
    "\n",
    "\n",
    "FLORIDA[\"df_clean\"] = load_and_clean_fema(\"fl_county.csv\")\n",
    "WASHINGTON[\"df_clean\"] = load_and_clean_fema(\"wa_county.csv\")\n",
    "\n",
    "# Preprocessing for GLM / Monte Carlo\n",
    "def preprocess_location_df(state_dict):\n",
    "    df = state_dict[\"df_clean\"].copy()\n",
    "\n",
    "    df[\"properties\"] = df[\"population\"] / AVERAGE_HOUSEHOLD_SIZE # proxy for no. of properties\n",
    "    df[\"insured_properties\"] = df[\"properties\"] * MARKET_SHARE # i.e. the \"exposure\" of the insurer\n",
    "    df[\"lambda_base\"] = df[\"insured_properties\"] * BASE_RATE # set a deterministic rate parameter for frequency\n",
    "    df[\"severity_mean\"] = df[\"EAL\"] / df[\"lambda_base\"] # set the deterministic severity parameter as FEMA's empirical Expected Annual Loss / frequency rate\n",
    "    df[\"risk_scaled\"] = df[\"RISK_VALUE\"] / df[\"RISK_VALUE\"].max() # rescale FEMA risk score to [0,1] as the original was [0,100]\n",
    "\n",
    "    state_dict[\"df_glm\"] = df\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "FLORIDA = preprocess_location_df(FLORIDA)\n",
    "WASHINGTON = preprocess_location_df(WASHINGTON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Parameter estimation and perturbation for underlying claim distributions\n",
    "\n",
    "The Frequency GLM models the claim counts $Y_i$ per county $i$ with the Poisson distribution with mean $\\mu_i$, which is the canonical choice for modeling count data in actuarial applications. \n",
    "\n",
    "$$ Y_i \\sim \\text{Poisson}(\\mu_i), \\; \\text{log}(\\mu_i)=\\text{log}(\\text{exposure}_i) + x_i\\beta$$\n",
    "$$\\implies \\mu_i =\\text{exposure}_i * \\text{exp}(x_i\\beta)$$\n",
    "\n",
    "Furthermore, the logarithmic link makes the predictor $x_i$ a coefficient that multiplies our rate $\\mu_i$, which is interpretable since the predictor $x_i$ is FEMA's proprietary risk score for each county, rescaled to $\\in[0,1]$. The exposure offset allows the rate to be modeled per unit of exposure, where exposure is the number of homes in a state times the market share of the insurer (i.e. the number of policies in their portfolio).\n",
    "\n",
    "Because actual claim count data is not publicly available, I construct a synthetic expected count using a baseline claim rate (CITE) and county exposures:\n",
    "\n",
    "$$ \\text{num claims}_i^{\\text{target}} = \\text{exposure}_i \\times \\text{base rate} \\times \\text{risk rel}_i $$\n",
    "\n",
    "Then, these baseline actuarial parameters derived by the GLMs are perturbed stochastically before Monte Carlo simulation:\n",
    "\n",
    "$$\\text{Poisson: } \\lambda_{\\text{sim}} = \\lambda_(\\text{base})(1+\\epsilon_\\lambda), \\epsilon_\\lambda \\sim \\mathcal{N}(0,\\sigma_\\lambda^2),$$\n",
    "$$\\text{Gamma: } \\mu_{\\text{sim}} = \\mu_(\\text{base})(1+\\epsilon_\\mu), \\epsilon_\\mu \\sim \\mathcal{N}(0,\\sigma_\\mu^2).$$\n",
    "\n",
    "This step introduces controlled uncertainty to reflect the heterogeneity of different counties in each state and model misspecification.\n",
    "\n",
    "The Severity GLM models the severity $X_i$ (i.e. cost) per claim with a Gamma GLM and log link:\n",
    "\n",
    "$$X_i \\sim \\text{Gamma}(\\mu_i, \\; \\phi \\mu_i^2), \\; log(\\mu_i)=x_i \\eta,$$\n",
    "\n",
    "where $\\phi$ is a scale parameter. Gamma is a good distribution choice for cost data because claim severities are positive, continuous, and right-skewed (reference 4). Under the log link, a unit increase in the predictor multiplies the expected severity by $e^{\\eta_1}$, holding all other covariates fixed.\n",
    "\n",
    "Both GLMs are estimated by IRLS with the GLM.fit() method. Finite-difference gradient checks are conducted and condition numbers of the design matrix are computed later to ensure the GLM outputs are well-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_glm_baselines(location):\n",
    "    \"\"\"\n",
    "    The previous synthetic baselines are now replaced by the fitted GLM means, incorporating both exposure and county risk scores.\n",
    "    \"\"\"\n",
    "    df = location[\"df_glm\"].copy()\n",
    "\n",
    "    # Frequency GLM (Poisson)\n",
    "    y_freq = df[\"lambda_base\"]\n",
    "\n",
    "    X_freq = sm.add_constant(df[[\"risk_scaled\"]])\n",
    "\n",
    "    freq_model = sm.GLM(\n",
    "        y_freq,\n",
    "        X_freq,\n",
    "        family=sm.families.Poisson(),\n",
    "        offset=np.log(df[\"insured_properties\"])\n",
    "    )\n",
    "\n",
    "    freq_results = freq_model.fit()\n",
    "\n",
    "    df[\"lambda_base\"] = freq_results.fittedvalues.clip(lower=0.01)\n",
    "\n",
    "    # Severity GLM (Gamma)\n",
    "    y_sev = df[\"severity_mean\"]\n",
    "    X_sev = sm.add_constant(df[[\"risk_scaled\"]])\n",
    "\n",
    "    sev_model = sm.GLM(\n",
    "        y_sev,\n",
    "        X_sev,\n",
    "        family=sm.families.Gamma(sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    sev_results = sev_model.fit()\n",
    "\n",
    "    # Replace severity mean with fitted mean\n",
    "    df[\"severity_mean\"] = sev_results.fittedvalues.clip(lower=1000)\n",
    "\n",
    "    # Store results (useful for diagnostics section)\n",
    "    location[\"df_glm\"] = df\n",
    "    location[\"glm_results\"] = {\n",
    "        \"frequency\": freq_results,\n",
    "        \"severity\": sev_results\n",
    "    }\n",
    "\n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, these baseline actuarial parameters derived by the GLMs are perturbed stochastically before Monte Carlo simulation:\n",
    "\n",
    "$$\\text{Poisson: } \\lambda_{\\text{sim}} = \\lambda_(\\text{base})(1+\\epsilon_\\lambda), \\epsilon_\\lambda \\sim \\mathcal{N}(0,\\sigma_\\lambda^2),$$\n",
    "$$\\text{Gamma: } \\mu_{\\text{sim}} = \\mu_(\\text{base})(1+\\epsilon_\\mu), \\epsilon_\\mu \\sim \\mathcal{N}(0,\\sigma_\\mu^2).$$\n",
    "\n",
    "This step introduces controlled uncertainty to reflect the heterogeneity of different counties in each state and model misspecification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>insured_properties</th>\n",
       "      <th>lambda</th>\n",
       "      <th>severity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alachua</td>\n",
       "      <td>5559.68</td>\n",
       "      <td>313.577026</td>\n",
       "      <td>429623.898001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baker</td>\n",
       "      <td>557.04</td>\n",
       "      <td>29.337465</td>\n",
       "      <td>488323.165830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bay</td>\n",
       "      <td>3497.38</td>\n",
       "      <td>201.589481</td>\n",
       "      <td>442977.102268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bradford</td>\n",
       "      <td>564.66</td>\n",
       "      <td>32.845276</td>\n",
       "      <td>392420.351588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brevard</td>\n",
       "      <td>12121.34</td>\n",
       "      <td>607.715664</td>\n",
       "      <td>402776.018916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     COUNTY  insured_properties      lambda  severity_mean\n",
       "0   Alachua             5559.68  313.577026  429623.898001\n",
       "1     Baker              557.04   29.337465  488323.165830\n",
       "2       Bay             3497.38  201.589481  442977.102268\n",
       "3  Bradford              564.66   32.845276  392420.351588\n",
       "4   Brevard            12121.34  607.715664  402776.018916"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def perturb_county_level_parameters(location, perturb_std_lambda=0.05, perturb_std_severity=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Produce county-level stochastic frequency (lambda) and severity.\n",
    "    \n",
    "    Args:\n",
    "        location: dict containing df_glm\n",
    "        perturb_std_lambda: relative std dev for lambda perturbation (5% default)\n",
    "        perturb_std_severity: relative std dev for severity perturbation (10% default)\n",
    "        random_state: seed for reproducibility\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = location[\"df_glm\"].copy()\n",
    "    \n",
    "    # Perturb lambda around base\n",
    "    df[\"lambda\"] = df[\"lambda_base\"] * (1 + rng.normal(0, perturb_std_lambda, size=len(df)))\n",
    "    df[\"lambda\"] = df[\"lambda\"].clip(lower=0.01)  # prevent negative lambda\n",
    "\n",
    "    # Perturb severity around mean\n",
    "    df[\"severity_mean\"] = df[\"severity_mean\"] * (1 + rng.normal(0, perturb_std_severity, size=len(df)))\n",
    "    df[\"severity_mean\"] = df[\"severity_mean\"].clip(lower=1000)  # prevent absurdly low severity\n",
    "\n",
    "    \n",
    "    location[\"df_sim\"] = df\n",
    "    return location\n",
    "\n",
    "# Apply GLM to both states\n",
    "FLORIDA = fit_glm_baselines(FLORIDA)\n",
    "WASHINGTON = fit_glm_baselines(WASHINGTON)\n",
    "\n",
    "# Apply stochastic perturbations now\n",
    "FLORIDA = perturb_county_level_parameters(FLORIDA, random_state=42)\n",
    "WASHINGTON = perturb_county_level_parameters(WASHINGTON, random_state=42)\n",
    "\n",
    "# Quick check of lambda and severity parameters after stochastic perturbation\n",
    "FLORIDA[\"df_sim\"][[\"COUNTY\", \"insured_properties\", \"lambda\", \"severity_mean\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Monte Carlo and Numerical Integration for simulating claims and profits\n",
    "\n",
    "The claim counts for each county on each Monte Carlo draw (N_SIM=10,000) is as follows:\n",
    "\n",
    "$$N \\sim \\text{Poisson}(\\lambda_{\\text{sim}})$$\n",
    "\n",
    "Instead of simulating individual claim severities, which would be computationally expensive and subject to Monte Carlo noise, the expected payout per claim is computed according to a standard policy scheme that includes a deductible $d$ and the coinsurance rate $c$:\n",
    "\n",
    "$$E[(X-d)^+] * c = c * \\int_d^{\\infty} (x-d)f_X(x)dx, \\; \\; X \\sim \\Gamma(k, \\; \\theta)$$\n",
    "\n",
    "A numerical integration approach is appropriate here because the deductible truncates the Gamma distribution, and so this integral has no closed-form expression. The implementation in the next cell evaluates the integral using n-point Gauss-Legendre quadrature:\n",
    "\n",
    "$$\\int_d^u g(x)dx \\approx \\sum_{i=1}^n w_i g(t_i),$$\n",
    "\n",
    "where $t_i, w_i$ are the Legendre nodes and weights mapped $[-1,1] \\to [d,u]$, and $u$, the 99.99th percentile of the Gamma distribution, is set as the upper bound of integration. This approach is a deterministic and numerically stable way of estimating the expected loss per claim.\n",
    "\n",
    "For each simulation trial,\n",
    "\n",
    "$$\\text{CountyLoss}_j = N_j * E[(X-d)^+] * c$$\n",
    "\n",
    "and the losses for each state are accumulated across each of its counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida sample losses (first 5 counties, 10000 draws):\n",
      "count    1.000000e+04\n",
      "mean     3.872161e+08\n",
      "std      1.128472e+07\n",
      "min      3.335626e+08\n",
      "25%      3.796284e+08\n",
      "50%      3.871139e+08\n",
      "75%      3.948156e+08\n",
      "max      4.345081e+08\n",
      "dtype: float64\n",
      "\n",
      "Washington sample losses (first 5 counties, 10000 draws):\n",
      "count    1.000000e+04\n",
      "mean     1.137988e+08\n",
      "std      5.313799e+06\n",
      "min      9.615461e+07\n",
      "25%      1.102478e+08\n",
      "50%      1.137569e+08\n",
      "75%      1.172989e+08\n",
      "max      1.372933e+08\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo Simulation with Quadrature for Severity\n",
    "\n",
    "def expected_payout_quadrature(deductible, coinsurance, shape, scale, n=200):\n",
    "    \"\"\"\n",
    "    Computes E[(X - deductible)^+] * coinsurance for Gamma(shape, scale) using n-point Gauss-Legendre quadrature.\n",
    "    \"\"\"\n",
    "    # Upper limit chosen as 99.99th percentile of severity distribution\n",
    "    upper = gamma.ppf(0.9999, a=shape, scale=scale)\n",
    "    \n",
    "    # Gauss–Legendre nodes & weights on [-1,1]\n",
    "    xs, ws = np.polynomial.legendre.leggauss(n)\n",
    "    \n",
    "    # Transform to [deductible, upper]\n",
    "    t = 0.5 * (xs + 1) * (upper - deductible) + deductible\n",
    "    w = 0.5 * (upper - deductible) * ws\n",
    "\n",
    "    # Integrand: (x - d) * f_X(x)\n",
    "    integrand = (t - deductible) * gamma.pdf(t, a=shape, scale=scale)\n",
    "    \n",
    "    # Multiply by coinsurance factor\n",
    "    return coinsurance * np.sum(w * integrand)\n",
    "\n",
    "\n",
    "def apply_deductible_coinsurance_vectorized(county_df, deductible=10000, coinsurance=0.8, n_sim=NUM_TRIALS, random_state=42):\n",
    "    \"\"\"\n",
    "    Monte Carlo simulation of state losses involves: Poisson frequency simulation and expected payout approximated by G-L quadrature\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    state_losses = np.zeros(n_sim)\n",
    "\n",
    "    for _, row in county_df.iterrows():\n",
    "        \n",
    "        # Simulate claim frequency per county\n",
    "        n_claims = rng.poisson(lam=row['lambda'], size=n_sim)\n",
    "\n",
    "        # Quadrature: expected payout per claim\n",
    "        severity_shape = 4  # can be tuned\n",
    "        severity_scale = row['severity_mean'] / severity_shape\n",
    "\n",
    "        E_payout = expected_payout_quadrature(deductible=deductible, coinsurance=coinsurance, shape=severity_shape, scale=severity_scale)\n",
    "\n",
    "        # Total losses per simulation draw\n",
    "        county_total = n_claims * E_payout\n",
    "\n",
    "        # Aggregate for state-level losses\n",
    "        state_losses += county_total\n",
    "\n",
    "    return state_losses\n",
    "\n",
    "\n",
    "# Quick sense check with first 5 counties\n",
    "FL_sim_losses = apply_deductible_coinsurance_vectorized(FLORIDA['df_sim'].head(5), n_sim=NUM_TRIALS)\n",
    "WA_sim_losses = apply_deductible_coinsurance_vectorized(WASHINGTON['df_sim'].head(5), n_sim=NUM_TRIALS)\n",
    "\n",
    "print(f\"Florida sample losses (first 5 counties, {NUM_TRIALS} draws):\")\n",
    "print(pd.Series(FL_sim_losses).describe())\n",
    "\n",
    "print(f\"\\nWashington sample losses (first 5 counties, {NUM_TRIALS} draws):\")\n",
    "print(pd.Series(WA_sim_losses).describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compute financial outcomes and Bootstrap Confidence Intervals\n",
    "\n",
    "Premiums are calculated by multiplying the expected loss in a state by 1.2 to each Monte Carlo simulation of each state's loss. Then profits are premiums - loss per simulation for each state, which yields an empirical distribution of potential financial outcomes. \n",
    "\n",
    "Using the empirical loss distribution from our simulations, risk measures like the Value at Risk and Tail Value at Risk are taken as the empirical 5th percentile of profit and the mean of the worst 5% of profit outcomes, respectively. \n",
    "\n",
    "A 95% confidence interval for the expected profit is constructed using nonparametric bootstrap resampling, which produces uncertainty bands without assuming normality of the simulated profit outcomes $\\set{\\Pi_1,...,\\Pi_n}$.\n",
    "\n",
    "$$ \\hat{\\mu}_b^* = \\frac{1}{n}\\sum_{i=1}^n \\Pi_i^{*(b)}, \\: b=1,...,B$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Florida Metrics ===\n",
      "Expected Loss: $387.22 M\n",
      "Premium:       $464.66 M\n",
      "Profit Mean:   $77.44 M ± $11.28 M (std)\n",
      "VaR 5%:        $58.68 M\n",
      "TVaR 5%:       $53.93 M\n",
      "Bootstrap 95% CI for mean profit: $77.22 M – $77.67 M\n",
      "\n",
      "\n",
      "=== Washington Metrics ===\n",
      "Expected Loss: $113.80 M\n",
      "Premium:       $136.56 M\n",
      "Profit Mean:   $22.76 M ± $5.31 M (std)\n",
      "VaR 5%:        $13.91 M\n",
      "TVaR 5%:       $11.54 M\n",
      "Bootstrap 95% CI for mean profit: $22.66 M – $22.86 M\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Financial computations and confidence interval bootstrapping\n",
    "\n",
    "def compute_metrics(state_losses, loading=1.2, n_bootstrap=NUM_TRIALS, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute premium, profit samples, VaR, TVaR, and bootstrap CI.\n",
    "    Inputs:\n",
    "        state_losses: array of total state-level losses (after deductible & coinsurance)\n",
    "        loading: premium multiplier\n",
    "        n_bootstrap: number of bootstrap resamples for mean profit CI\n",
    "    Returns:\n",
    "        metrics: dict with expected loss, premium, VaR, TVaR, profit samples, bootstrap CI\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    state_losses = np.array(state_losses)\n",
    "    \n",
    "    # Expected loss and premium\n",
    "    expected_loss = state_losses.mean()\n",
    "    premium = expected_loss * loading\n",
    "    \n",
    "    # Profit samples\n",
    "    profit_samples = premium - state_losses\n",
    "    \n",
    "    # Risk metrics\n",
    "    VaR_95 = np.percentile(profit_samples, 5) # Value at Risk\n",
    "    TVaR_95 = profit_samples[profit_samples <= VaR_95].mean() # Tail Value at Risk for worst 5% of profit outcomes\n",
    "    \n",
    "    # Bootstrap 95% Confidence Interval for mean profit\n",
    "    bootstrap_means = np.array([rng.choice(profit_samples, size=len(profit_samples), replace=True).mean() for _ in range(n_bootstrap)])\n",
    "    ci_lower, ci_upper = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "    \n",
    "    metrics = {\n",
    "        'expected_loss': expected_loss,\n",
    "        'premium': premium,\n",
    "        'VaR_95': VaR_95,\n",
    "        'TVaR_95': TVaR_95,\n",
    "        'profit_mean': profit_samples.mean(),\n",
    "        'profit_std': profit_samples.std(),\n",
    "        'profit_bootstrap_CI': (ci_lower, ci_upper)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, state_name):\n",
    "    \"\"\"\n",
    "    Nicely format and print financial metrics for a given state\n",
    "    \"\"\"\n",
    "    print(f\"=== {state_name} Metrics ===\")\n",
    "    print(f\"Expected Loss: ${metrics['expected_loss']/1e6:,.2f} M\")\n",
    "    print(f\"Premium:       ${metrics['premium']/1e6:,.2f} M\")\n",
    "    print(f\"Profit Mean:   ${metrics['profit_mean']/1e6:,.2f} M ± ${metrics['profit_std']/1e6:,.2f} M (std)\")\n",
    "    print(f\"VaR 5%:        ${metrics['VaR_95']/1e6:,.2f} M\")\n",
    "    print(f\"TVaR 5%:       ${metrics['TVaR_95']/1e6:,.2f} M\")\n",
    "    ci_lower, ci_upper = metrics['profit_bootstrap_CI']\n",
    "    print(f\"Bootstrap 95% CI for mean profit: ${ci_lower/1e6:,.2f} M – ${ci_upper/1e6:,.2f} M\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Apply metrics to each state\n",
    "FL_metrics = compute_metrics(FL_sim_losses)\n",
    "WA_metrics = compute_metrics(WA_sim_losses)\n",
    "\n",
    "# Print nicely for both states\n",
    "print_metrics(FL_metrics, \"Florida\")\n",
    "print_metrics(WA_metrics, \"Washington\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLM Diagnostics\n",
    "\n",
    "This cell conducts a diagnostic check on the fitted frequency and severity GLMs, including:\n",
    "\n",
    "- Displaying a full GLM statistics summary for the Poisson (frequency) and Gamma (severity) GLM models\n",
    "- Computing design matrix condition numbers to assess numerical stability\n",
    "- Performs finite-difference gradient checks to validate whether the design matrices is well-conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Frequency (Poisson) GLM Diagnostics\n",
      "======================================================================\n",
      "\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            lambda_base   No. Observations:                   67\n",
      "Model:                            GLM   Df Residuals:                       65\n",
      "Model Family:                 Poisson   Df Model:                            1\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -223.34\n",
      "Date:                Fri, 12 Dec 2025   Deviance:                  -4.8675e-13\n",
      "Time:                        23:02:28   Pearson chi2:                 1.70e-25\n",
      "No. Iterations:                     6   Pseudo R-squ. (CS):         -1.985e-13\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -2.8904      0.010   -288.043      0.000      -2.910      -2.871\n",
      "risk_scaled -1.055e-15      0.019  -5.68e-14      1.000      -0.036       0.036\n",
      "===============================================================================\n",
      "\n",
      "Design matrix describe():\n",
      "         x0         x1\n",
      "count  67.0  67.000000\n",
      "mean    1.0   0.126375\n",
      "std     0.0   0.200739\n",
      "min     1.0   0.003695\n",
      "25%     1.0   0.015573\n",
      "50%     1.0   0.058471\n",
      "75%     1.0   0.146869\n",
      "max     1.0   1.000000\n",
      "\n",
      "Design matrix condition number:\n",
      "5.102598455082846\n",
      "\n",
      "Gradient comparison:\n",
      "Finite-difference gradient: [3.7222222222910157 0.4703942848571784]\n",
      "Analytical gradient:        [0. 0.]\n",
      "\n",
      "======================================================================\n",
      "Severity (Gamma, log link) GLM Diagnostics\n",
      "======================================================================\n",
      "\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:          severity_mean   No. Observations:                   67\n",
      "Model:                            GLM   Df Residuals:                       65\n",
      "Model Family:                   Gamma   Df Model:                            1\n",
      "Link Function:                    log   Scale:                         0.25348\n",
      "Method:                          IRLS   Log-Likelihood:                -913.78\n",
      "Date:                Fri, 12 Dec 2025   Deviance:                       16.594\n",
      "Time:                        23:02:28   Pearson chi2:                     16.5\n",
      "No. Iterations:                    10   Pseudo R-squ. (CS):          0.0001240\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          13.0163      0.073    178.701      0.000      12.874      13.159\n",
      "risk_scaled     0.0293      0.309      0.095      0.924      -0.576       0.634\n",
      "===============================================================================\n",
      "\n",
      "Design matrix describe():\n",
      "         x0         x1\n",
      "count  67.0  67.000000\n",
      "mean    1.0   0.126375\n",
      "std     0.0   0.200739\n",
      "min     1.0   0.003695\n",
      "25%     1.0   0.015573\n",
      "50%     1.0   0.058471\n",
      "75%     1.0   0.146869\n",
      "max     1.0   1.000000\n",
      "\n",
      "Design matrix condition number:\n",
      "5.102598455082846\n",
      "\n",
      "Gradient comparison:\n",
      "Finite-difference gradient: [30240862.385369834  3857178.686186671]\n",
      "Analytical gradient:        [0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "/Users/junomarquesoda/anaconda3/lib/python3.11/site-packages/statsmodels/genmod/generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-5 # for finite-difference analysis\n",
    "\n",
    "def finite_difference_gradient(X, beta, mu_transform, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Finite-difference gradient approximation for GLM mean function\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(beta)\n",
    "\n",
    "    for i in range(len(beta)):\n",
    "        beta_plus = beta.copy()\n",
    "        beta_minus = beta.copy()\n",
    "        beta_plus[i] += epsilon\n",
    "        beta_minus[i] -= epsilon\n",
    "        mu_plus = mu_transform(X @ beta_plus)\n",
    "        mu_minus = mu_transform(X @ beta_minus)\n",
    "\n",
    "        grad[i] = (mu_plus.sum() - mu_minus.sum()) / (2 * epsilon)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def glm_diagnostics(results, X, y, model_name, mu_transform, analytical_grad):\n",
    "    \"\"\"\n",
    "    Print GLM summary, design matrix diagnostics, condition number, and finite-difference vs analytical gradient checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name} GLM Diagnostics\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Model summary\n",
    "    print(results.summary())\n",
    "\n",
    "    # Design matrix diagnostics\n",
    "    X_df = pd.DataFrame(X, columns=[f\"x{i}\" for i in range(X.shape[1])])\n",
    "    print(\"\\nDesign matrix describe():\")\n",
    "    print(X_df.describe())\n",
    "\n",
    "    print(\"\\nDesign matrix condition number:\")\n",
    "    print(cond(X))\n",
    "\n",
    "    # Gradient checks\n",
    "    beta = results.params.values\n",
    "    mu_hat = results.fittedvalues.values\n",
    "\n",
    "    grad_fd = finite_difference_gradient(\n",
    "        X=X,\n",
    "        beta=beta,\n",
    "        mu_transform=mu_transform,\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "\n",
    "    grad_an = analytical_grad(X, y, mu_hat)\n",
    "\n",
    "    print(\"\\nGradient comparison:\")\n",
    "    print(\"Finite-difference gradient:\", grad_fd)\n",
    "    print(\"Analytical gradient:       \", grad_an)\n",
    "\n",
    "# Run diagnostics on FLORIDA\n",
    "df = FLORIDA[\"df_glm\"]\n",
    "\n",
    "#Frequency GLM (Poisson)\n",
    "freq_results = FLORIDA[\"glm_results\"][\"frequency\"]\n",
    "\n",
    "X_freq = sm.add_constant(df[[\"risk_scaled\"]]).values\n",
    "y_freq = df[\"lambda_base\"].values\n",
    "\n",
    "glm_diagnostics(results=freq_results, X=X_freq, y=y_freq, model_name=\"Frequency (Poisson)\", mu_transform=np.exp, analytical_grad=lambda X, y, mu: X.T @ (y - mu))\n",
    "\n",
    "# Severity GLM (Gamma, log link)\n",
    "sev_results = FLORIDA[\"glm_results\"][\"severity\"]\n",
    "\n",
    "X_sev = sm.add_constant(df[[\"risk_scaled\"]]).values\n",
    "y_sev = df[\"severity_mean\"].values\n",
    "\n",
    "glm_diagnostics(results=sev_results, X=X_sev, y=y_sev, model_name=\"Severity (Gamma, log link)\", mu_transform=np.exp, analytical_grad=lambda X, y, mu: X.T @ ((y - mu) / (mu ** 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b80b5fa210c79401494629cf98f1efa3",
     "grade": false,
     "grade_id": "cell-8139016f051ab235",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Discussion [15 pts]\n",
    "\n",
    "Evaluate the results of your project including \n",
    "* Why should I believe that your numerical results are correct (convergence, test cases etc)?\n",
    "* Did the project work (in your opinion)?\n",
    "* If yes:  what would be the next steps to try\n",
    "* If no:  Explain why your approach did not work and what you might do to fix it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e348f6f1cc23d5fe7ed09d36bfc1c4e",
     "grade": true,
     "grade_id": "cell-596162f90cd1e909",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "I will now evaluate whether the value and efficiency of the project by evaluating the sensibility of the model's outputs and diagnostics, as well as checking whether each computational stage in the pipeline was executed appropriately and contributed to the results and interpretability of the model.\n",
    "\n",
    "Beginning with the numerical results from the last diagnostics cell, it seems that the Frequency GLM, based on a Poisson distribution, has perfect separation due to the Warning that the code throws. This is likely because the synthetic frequency rate lambda_base was derived deterministically from the start, so the GLM was unable to find any signal in the predictor because the target is almost entirely determined by the offset term (see Methodology section). This is evidenced by the fitted coefficient for the predictor risk_scaled being nearly zero (8.465E-16) and having a p-value of 1.0. The same line of critique applies to the Severity GLM, as the perfect separation warning is also raised.\n",
    "\n",
    "However, the numerical methods themselves were executed correctly, as the IRLS algorithm converged in 6 iterations, the condition numbers of $\\approx 5.1$ do not raise any concerns that the design matrices are ill-conditioned, and the Monte Carlo simulations produced reasonable magnitudes of losses, premiums, and profits for states like Florida and Washington. For example, it makes sense that Florida would experience much more losses than Washington, just due to the size of each state. At the same time, the magnitudes of state-level losses being in the range of hundreds of millions could be reasonable for an insurer whose market share is 5% of each state. Still, the finite-difference gradient analysis for both GLMs is nonzero, which indicates numerical instability and that the GLMs do not meaningfully capture much variation across counties in each state with the current covariates.\n",
    "\n",
    "I think this project did work- the pipeline of fitting GLMs, perturbing parameters, running Monte Carlo simulations and approximations with numerical integration, and computing profit and risk metrics, were all fully functional and interpretable steps in the end-to-end insurance model I set out to develop. While the modelling logic was sound, I think my GLMs ended up being degenerate because the FEMA data did not provide enough signal in the covariates to be able to meaningfully capture variations in the claim frequency and severities.\n",
    "\n",
    "The next steps include either finding or simulating real-world claim count data to fit the Frequency GLM on, because it clearly was not able to learn on the synthetically constructed, deterministic frequency and severity parameters. In addition, more covariates could be added to the GLM models, such as population density and historical loss ratios (a commonly tracked metric by insurers). And since there are a few global variables (i.e. model hyperparameters) at play in this project, such as the baseline claim rate and insurer market share, sensitivity analyses could be conducted to measure the response of state-level profit and loss metrics. Furthermore, the spirit of catastrophe modeling could be infused into this insurance model by stochastically perturbing the severity distributions with more than just Gaussian noise, in order to represent extreme events like hurricanes in Florida or flooding in Washington. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
